{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "\n",
    "pandas_df = pd.read_pickle(\"./raw_weekly_df.pkl\")\n",
    "dask_df = dd.from_pandas(pandas_df, npartitions=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe only place where we can save memory is potentially changing the float32 column to a less data intensive float \\ntype. Everything else is a category or datetime.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The only place where we can save memory is potentially changing the float32 column to a less data intensive float \n",
    "type. Everything else is a category or datetime.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downcast in order to save memory\n",
    "#TODO comments added in .py\n",
    "def downcast(df):\n",
    "    cols = df.dtypes.index.tolist()\n",
    "    types = df.dtypes.values.tolist()\n",
    "    for i,t in enumerate(types):\n",
    "        if 'int' in str(t):\n",
    "            df[cols[i]] = pd.to_numeric(df[cols[i]], downcast='integer')\n",
    "        elif 'float' in str(t):\n",
    "            df[cols[i]] = pd.to_numeric(df[cols[i]], downcast='float')\n",
    "        elif t == np.object:\n",
    "            if cols[i] == 'date':\n",
    "                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype('category')\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe above downcast won't actually help, since the only place where our data can save any memory is float, and \\nthe above method just downcasts it to a float32, which is already the case. But this could be useful for future \\ndatasets. \\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The above downcast won't actually help, since the only place where our data can save any memory is float, and \n",
    "the above method just downcasts it to a float32, which is already the case. But this could be useful for future \n",
    "datasets. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6841121 entries, 0 to 6841120\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Dtype         \n",
      "---  ------    -----         \n",
      " 0   dept_id   category      \n",
      " 1   cat_id    category      \n",
      " 2   item_id   category      \n",
      " 3   state_id  category      \n",
      " 4   store_id  category      \n",
      " 5   datetime  datetime64[ns]\n",
      " 6   sales     float32       \n",
      "dtypes: category(5), datetime64[ns](1), float32(1)\n",
      "memory usage: 117.5 MB\n"
     ]
    }
   ],
   "source": [
    "downcast(pandas_df).info(memory_usage=\"Deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO when splitting up tasks in multiple functions, ask if it makes the code easier or harder to read. \n",
    "# IMO, these splits make the code harder to understand\n",
    "    \n",
    "    \n",
    "#I tried upsampling in order to find the weeks that are missing by adding those rows in and filling them in with NaN\n",
    "#However, for some reason, after '2011-01-31' instead of '2011-02-07', it starts with '2011-02-06' so it throws it off\n",
    "#TODO can you show me screenshots of what you mean? the data was aggregated using W-MON format, maybe that will solve your issue\n",
    "#TODO don't hardcode things like date formats since they're likely to change\n",
    "#TODO don't need parentheses around return (minor style issue)\n",
    "\n",
    "\n",
    "#The following function is quite inefficient since it has quadratic runtime O(n^2), but I'm not sure how to fix this\n",
    "def find_missing_weeks_in_entire_dataframe(dataframe):\n",
    "    #TODO if you'll never use any of these functions again, define them within the scope of this function so they aren't hanging out in memory\n",
    "    \n",
    "    def find_item_and_store_combo(dataframe, store_id, item_id):\n",
    "        #TODO nice job on the descriptive variable names!\n",
    "        #TODO hardcoding\n",
    "        return dataframe[(dataframe['store_id'] == store_id) & (dataframe['item_id'] == item_id)]\n",
    "    \n",
    "    def upsampling(dataframeinput):\n",
    "        return dataframeinput.set_index('datetime').resample('w').asfreq()\n",
    "    \n",
    "    #TODO hardcoding\n",
    "    def find_missing_weeks_for_specific_combo(dataframeinput):\n",
    "        return dataframeinput[dataframeinput['dept_id'].isna()]\n",
    "\n",
    "    combo_dict = dataframe.groupby(['store_id', 'item_id']).groups.keys()\n",
    "    for key in combo_dict: \n",
    "        combo = find_item_and_store_combo(dataframe, key[0], key[1])\n",
    "        upsampled = upsampling(combo)\n",
    "        #print(find_missing_weeks_for_specific_combo(upsampled))\n",
    "\n",
    "find_missing_weeks_in_entire_dataframe(pandas_df)\n",
    "#x = ((pandas_df.groupby(['store_id', 'item_id'])['datetime']).count())\n",
    "#print(x.head(10))\n",
    "#print(pandas_df.groupby(['store_id', 'item_id']).groups.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO move all graphing functions to /src/exploration.py\n",
    "#TODO nice job with the graphs! now tell me how to interpret them (what insights can you glean from what's shown below?)\n",
    "def univariate_sales_histogram(pandadataframe):\n",
    "    sales_series = pandadataframe['sales']\n",
    "    max_sale = sales_series.max()\n",
    "    (pandadataframe.hist(column='sales',bins=100, figsize=(12,8), range=[0,50]))\n",
    "    \n",
    "def bivariate_sales_by_state(pandadataframe):\n",
    "    sales_series = pandadataframe['sales']\n",
    "    max_sale = sales_series.max()\n",
    "    pandadataframe.hist(column='sales', by=\"state_id\",bins=100, figsize=(12,8), range=[0,50])\n",
    "\n",
    "def bivariate_sales_by_store(pandadataframe):\n",
    "    sales_series = pandadataframe['sales']\n",
    "    max_sale = sales_series.max()\n",
    "    pandadataframe.hist(column='sales', by=\"store_id\",bins=100, figsize=(12,8), range=[0,50])\n",
    "\n",
    "univariate_sales_histogram(pandas_df)\n",
    "bivariate_sales_by_state(pandas_df)\n",
    "bivariate_sales_by_store(pandas_df)\n",
    "\n",
    "\"\"\"\n",
    "sales_series = pandas_df['sales']\n",
    "max_sale = sales_series.max()\n",
    "print(max_sale)\n",
    "ax = pandas_df.hist(column='sales', by=\"state_id\",bins=100, figsize=(12,8), range=[0,50])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
